The purpose of this repo is a full documented LLM project which copied from nano-vllm but replace the engine from QWEN to LLAMA.

Install flash-attention

MAX_JOBS=2 pip install flash-attn --no-build-isolation